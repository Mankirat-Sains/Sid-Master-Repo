# -*- coding: utf-8 -*-
"""Copy of Projects Master JSON.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FLf-IS2OsjdTcrmYESQZU6A75I87jPql
"""

# @title CELL 0: SETUP & DEPENDENCIES
!pip install openai pandas opencv-python tqdm

from google.colab import drive
import os
import sys

# Mount Google Drive
drive.mount('/content/drive')

# Define your Root Path (The "Master" folder)
# Make sure this matches your folder structure exactly!
ROOT_DIR = "/content/drive/MyDrive/Projects Master JSON/dataset"

if not os.path.exists(ROOT_DIR):
    print(f"‚ùå ERROR: Could not find directory: {ROOT_DIR}")
    print("Please check your Google Drive folder structure.")
else:
    print(f"‚úÖ SUCCESS: Found Root Directory: {ROOT_DIR}")

# @title CREATE NEW PROJECT FOLDER SCRIPT
import os

print("--- BATCH PROJECT CREATOR STARTED ---")
print("Type a project code to create folders, or 'n' to stop.")

while True:
    # 1. Ask user for the Project Code
    print("\n" + "="*30)
    user_input = input("Enter Project Code (e.g., 25-01-039): ").strip()

    # 2. Check for exit condition
    if user_input.lower() == 'n':
        print("\nüõë Process stopped by user.")
        break

    # 3. Validate input
    if not user_input:
        print("‚ùå Error: No input detected. Please try again.")
        continue

    # 4. Construct the full path
    folder_name = f"Project {user_input}"
    project_path = os.path.join(ROOT_DIR, folder_name)

    # 5. Define the subfolders to create
    subfolders = ["labels", "images", "crops"]

    # 6. Check existence and Create
    if os.path.exists(project_path):
        print(f"‚ö†Ô∏è  WARNING: '{folder_name}' already exists.")
        print("    Skipping to protect existing data...")
    else:
        try:
            # Create the main project folder
            os.makedirs(project_path, exist_ok=True)
            print(f"üìÇ Created: {folder_name}")

            # Create the subfolders
            for sub in subfolders:
                os.makedirs(os.path.join(project_path, sub), exist_ok=True)
                print(f"   ‚îú‚îÄ‚îÄ /{sub}")

            print(f"‚úÖ SUCCESS: Structure ready for {user_input}")

        except Exception as e:
            print(f"‚ùå ERROR: Could not create folders. Reason: {e}")

# @title VERIFY IMG COUNT IN FOLDER SCRIPT
import os

print(f"üîé Scanning Root Directory: {ROOT_DIR}\n")
print(f"{'PROJECT NAME':<30} | {'IMAGES':<8} | {'LABELS':<8} | {'CROPS':<8}")
print("-" * 65)

# Get a sorted list of folders in the dataset directory
try:
    projects = sorted([p for p in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, p))])

    if not projects:
        print("‚ùå No project folders found in the dataset directory.")

    for project in projects:
        project_path = os.path.join(ROOT_DIR, project)

        # Define paths to the specific subfolders
        # Using 'crops' (plural) to match the creation script
        img_path = os.path.join(project_path, "images")
        lbl_path = os.path.join(project_path, "labels")
        crp_path = os.path.join(project_path, "crops")

        # Function to safely count files (returns 0 if folder missing)
        def count_files(path):
            if os.path.exists(path):
                # Count only files, ignore hidden .ipynb_checkpoints if they exist
                return len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and not f.startswith('.')])
            return 0

        # Get counts
        n_images = count_files(img_path)
        n_labels = count_files(lbl_path)
        n_crops  = count_files(crp_path)

        # Print row
        print(f"{project:<30} | {n_images:<8} | {n_labels:<8} | {n_crops:<8}")

except NameError:
    print("‚ùå ERROR: ROOT_DIR is not defined. Please run the 'SETUP' cell first.")
except Exception as e:
    print(f"‚ùå An error occurred: {e}")

# @title CELL 1: THE RECURSIVE CROPPER
import cv2
import glob
from tqdm import tqdm
import re
import shutil

print("üöÄ Starting Recursive Cropper...")

# 1. FIND ALL PROJECT FOLDERS
# We look for any folder inside the ROOT_DIR
project_folders = [f.path for f in os.scandir(ROOT_DIR) if f.is_dir()]

print(f"üìÇ Found {len(project_folders)} projects to process.")

for project_path in project_folders:
    project_name = os.path.basename(project_path)
    print(f"\n--- Processing Project: {project_name} ---")

    # Define Sub-paths
    images_dir = os.path.join(project_path, "images")
    labels_dir = os.path.join(project_path, "labels")
    output_dir = os.path.join(project_path, "crops") # We create crops INSIDE the project folder

    # Validation
    if not os.path.exists(images_dir) or not os.path.exists(labels_dir):
        print(f"‚ö†Ô∏è Skipping {project_name}: Missing 'images' or 'labels' folder.")
        continue

    # Reset/Create Crops Folder
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir) # Clean start
    os.makedirs(output_dir, exist_ok=True)

    # Get Label Files
    label_files = glob.glob(os.path.join(labels_dir, "*.txt"))
    print(f"   Found {len(label_files)} label files.")

    crop_count = 0

    for label_file in label_files:
        # Get messy name (e.g. "00c9719a-S2-IMG1")
        base_name = os.path.basename(label_file).replace(".txt", "")

        # CLEAN NAME (Remove 8-char hash)
        clean_name = re.sub(r'^[a-zA-Z0-9]{8}-', '', base_name)

        # Find Image (Try PNG then JPG)
        # We search using the ORIGINAL name because the raw image file still has the hash
        image_path = os.path.join(images_dir, base_name + ".png")
        if not os.path.exists(image_path):
            image_path = os.path.join(images_dir, base_name + ".jpg")

        # Fallback: Try finding by clean name
        if not os.path.exists(image_path):
             image_path = os.path.join(images_dir, clean_name + ".png")

        if not os.path.exists(image_path):
            # print(f"   ‚ö†Ô∏è Image not found for: {base_name}")
            continue

        # Load Image
        img = cv2.imread(image_path)
        if img is None: continue
        h_img, w_img, _ = img.shape

        # Read Coordinates
        with open(label_file, "r") as f:
            lines = f.readlines()

        for i, line in enumerate(lines):
            parts = line.strip().split()
            if len(parts) < 5: continue

            x_c, y_c, w, h = map(float, parts[1:5])

            # YOLO to Pixel
            x1 = int((x_c - w/2) * w_img)
            y1 = int((y_c - h/2) * h_img)
            x2 = int((x_c + w/2) * w_img)
            y2 = int((y_c + h/2) * h_img)

            # Clamp
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w_img, x2), min(h_img, y2)

            if x2 <= x1 or y2 <= y1: continue

            # Crop & Save
            crop = img[y1:y2, x1:x2]

            # SAVE AS: ProjectName_CleanName_CropID.png
            # (Adding Project Name ensures uniqueness later if we mix them, though folders separate them)
            # Actually, let's keep it simple: CleanName_CropID.png inside the folder
            crop_filename = f"{clean_name}_crop_{i}.png"
            cv2.imwrite(os.path.join(output_dir, crop_filename), crop)
            crop_count += 1

    print(f"   ‚úÖ Created {crop_count} crops in {output_dir}")

print("\n‚ú® PHASE 1 COMPLETE: All projects cropped.")

# @title CELL 2.1: THE EXTRACTOR (Visual -> JSON)
import base64
import json
from openai import OpenAI
import os
import glob
import time
import re

# ================= CONFIGURATION =================
API_KEY = os.getenv("OPENAI_API_KEY", "YOUR_API_KEY_HERE")  # Set OPENAI_API_KEY environment variable or replace with your key
ROOT_DIR = "/content/drive/MyDrive/Projects Master JSON/dataset"
BATCH_SIZE = 7

# =================================================
# üìã PASTE YOUR "GOLDEN PROMPT 3 - PART 1" BELOW
# =================================================
GOLDEN_PROMPT_PART_1 = r"""
You are a senior structural engineer and a vision-language model.
You have access to technical drawing images (cropped details).
Your task is to analyze these images and produce deeply detailed, engineering-aware JSON representations.

You MUST follow all rules below with zero exceptions.

====================================================================
GLOBAL RULES
====================================================================
1. **Be extremely explicit, structured, and exhaustive.**
2. **Extract ALL visible text** exactly as shown: callouts, notes, dimensions, gridlines, elevations, symbols, tags, material labels, detail references, sheet references, etc.
3. **Describe the image visually AND semantically** (what engineering concepts it represents).
4. **Infer structural intent whenever possible**, including:
    - Gravity load path
    - Lateral load path (shear walls, braces, diaphragms)
    - Structural roles of elements
5. **Output ONLY JSON. No commentary.**

====================================================================
OUTPUT FORMAT
====================================================================
Output a JSON object with a SINGLE key: "images".
The value must be an array of objects following the "Images Array Schema" below.

{
  "images": [
      { ... image 1 JSON ... },
      { ... image 2 JSON ... }
  ]
}

====================================================================
SECTION 1: IMAGES ARRAY SCHEMA
====================================================================

For each image, produce the following JSON object:
You must strictly follow the format of these distinct parts:

{
  "image_id": "<unique id, filename>",
  "caption_detailed": "<Concise engineering narrative: view type (Plan/Section/Schedule), system visible, and code context>",

  // 1Ô∏è‚É£ PARENT PAGE IDENTITY (Source of Truth)
  // This object anchors the crop to its original PDF page.
  "source_page_identity": {
      // 1. VISUAL EXTRACTION: Look at the title block text.
      "page_code": "<e.g. 'A2.01', 'S1', 'S5.05'. If not found, return null.>",
      "page_description": "<e.g. 'BASEMENT FLOOR PLAN', 'BRACING DETAILS'. If not found, return null.>",

      // 2. FILENAME EXTRACTION: MANDATORY.
      // Extract the numeric page index explicitly written in the 'image_id' filename.
      // Example: If filename is 'Dave_Matthews...page_6_crop_0.jpg', return '6'.
      "page_index": "<The integer page number derived strictly from the filename string.>"
  },


  // 2Ô∏è‚É£ TEXT & DATA EXTRACTION
  // INSTRUCTION: Extract EVERY piece of text visible on the crop.
  // Sort into one of these three categories. Do not summarize; use verbatim strings.
  "exact_text": {

      // A. PARAGRAPHS & LISTS
      // Text that appears in blocks, lists, or large notes sections.
      "general_notes_and_legends": [
          {
             "header_or_title": "<e.g. GENERAL NOTES / LEGEND / SITE PLAN NOTES>",
             "content_verbatim": [
                 "<e.g. '1. ALL FASTENERS TO BE ACQ RATED.'>",
                 "<e.g. '2. VERIFY DIMENSIONS PRIOR TO FABRICATION.'>"
             ]
          }
      ],

      // B. STRUCTURED GRIDS
      // Text found inside bordered tables or schedules.
      "schedules_and_tables": [
          {
             "table_title": "<e.g. LINTEL SCHEDULE / BEAM SCHEDULE>",
             "rows": [
                 // Capture every cell in the row
                 {
                   "id": "<e.g. L1>",
                   "col_1": "<e.g. 2-2x6 HEADER>",
                   "col_2": "<e.g. 2 JACKS>",
                   "col_3": "<e.g. ENGINEERING STAMP>"
                 }
             ]
          }
      ],

      // C. SPATIAL LABELS & ANNOTATIONS
      // INSTRUCTION: Create a separate object for EVERY text label found.
      // Do not group them. (e.g., if there are 5 dimensions, list 5 separate objects).
      "isolated_labels_and_dimensions": [
          {
             "text_content": "<e.g. 'MASTER BEDROOM'>",
             "label_type": "<Room Label>",
             "position_context": "<e.g. Center of large room on East side>"
          },
          {
             "text_content": "<e.g. 'ENSUITE'>",
             "label_type": "<Room Label>",
             "position_context": "<e.g. Adjacent to Master Bedroom>"
          },
          {
             "text_content": "<e.g. '12'-0\"'>",
             "label_type": "<Dimension>",
             "position_context": "<e.g. Width of Master Bedroom>"
          },
          {
             "text_content": "<e.g. '4'-6\"'>",
             "label_type": "<Dimension>",
             "position_context": "<e.g. Width of Walk-In Closet>"
          },
          {
             "text_content": "<e.g. 'W8x15'>",
             "label_type": "<Structural Tag>",
             "position_context": "<e.g. Label pointing to steel beam>"
          },
          {
             "text_content": "<e.g. 'SIM.' or 'TYP.'>",
             "type": "<Annotation>",
             "nearby_visual_context": "<Near joist spacing>"
          }
      ]
  },

  // 3Ô∏è‚É£ SEMANTIC ENTITIES (Consolidated Source of Truth)
  "semantic_entities": {
      // A. DISCRETE MEMBERS (The "Skeleton")
      // Items that are distinct, countable structural objects.
      "discrete_structural_members": [
          {
             "category": "<Column / Beam / Truss / Footing / Lintel / Header>",
             "tag_label": "<e.g., B1, C4, L2 (The ID used in schedules)>",
             "raw_specification": "<e.g., '3-ply 2x10 SPF', 'W8x15', '15M Rebar'>",
             "dimensions_local": "<Specific size of THIS member, e.g., '9 ft long'>",
             "elevation_context": "<e.g., 'Below Main Floor', 'Flush with Ridge'>",
             "structural_function": "<Gravity Support / Lateral Bracing / Point Load Transfer>",
             "engineering_notes": "<e.g., 'Pocketed into foundation wall', 'Bearing on 2x6 wall'>"
          }
      ],

      // B. ASSEMBLIES & LAYERS (The "Skin & Buildup")
      // Captures Wall/Floor/Roof stacks (e.g. from your 'Wall Assemblies' tables).
      "assemblies_and_layers": [
          {
             "assembly_id": "<e.g., EW1, FL2, ICF1>",
             "assembly_type": "<e.g., Exterior Wall, Slab-on-Grade, Roof>",
             "layer_composition": [
                 "<e.g., 'Siding per client'>",
                 "<e.g., '1 inch Rigid Insulation'>",
                 "<e.g., '2x6 Studs @ 16 o.c.'>",
                 "<e.g., '6 mil Poly Vapor Barrier'>"
             ],
             "bulk_materials_identified": ["<Concrete>", "<Gravel>", "<XPS Foam>"]
          }
      ],

      // C. GLOBAL DIMENSIONS & GRIDS (The "Geometry")
      // Dimensions that define the BUILDING, not just one beam.
      "global_dimensions": [
          {
             "dimension_value": "<e.g., 52'-0\", 14'-0\">",
             "measurement_type": "<Overall Building Width / Bay Spacing / Grid-to-Grid>",
             "orientation": "<Horizontal / Vertical / Diagonal>",
             "reference_points": ["<Start Point: Grid A>", "<End Point: Grid B>"]
          }
      ],

      // D. CONNECTIONS & NODES (The "Joints")
      // Where two distinct members meet.
      "connections_and_joints": [
          {
             "connection_type": "<e.g., Hanger, Lap Splice, Base Plate Anchor>",
             "participating_elements": ["<Member A>", "<Member B>"],
             "visual_evidence": "<e.g., 'Simpson Hanger visible', 'Dashed outline of bolt'>",
             "engineering_purpose": "<e.g., 'Transfer shear from beam to column'>"
          }
      ],

      // E. SYMBOLS & ANNOTATIONS (The "map")
      // Callouts that point to OTHER drawings.
      "symbols_and_callouts": [
          {
             "symbol_type": "<Section Cut / Detail Bubble / Elevation Marker / Pitch Triangle>",
             "label_text": "<e.g., 'A', '3/D1', '12/6' (pitch)>",
             "references_external_view": "<Yes/No - does this point to another sheet?>",
             "meaning": "<e.g., 'See Detail 3 on Sheet D1', 'North Elevation View'>"
          }
      ]
  },

  // 3Ô∏è‚É£ Engineering Understanding
  "engineering_understanding": {
        "gravity_system": {
            "load_path_explanation": "<e.g., roof trusses -> bearing walls -> slab/foundations>",
            "key_elements": ["<members involved>"]
        },
        "lateral_system": {
            "identified_elements": ["<shearwalls, braces, diaphragms>"],
            "function_explanation": "<why these elements resist wind/seismic loads>"
        },
        "purpose_of_detail": "<why this detail exists>",
        "potential_engineering_rationale": "<design decisions likely made>"
  },


  // 4Ô∏è‚É£ Project Metadata (Global Info visible on this crop)
  "project_metadata": {
      "project_name": "<if visible>",
      "client_name": "<if visible>",
      "project_location": "<if visible>",
      "building_type": "<barn / agricultural / industrial / unknown>",
      "date": "<if shown>"
  }
}
"""
# =================================================

client = OpenAI(api_key=API_KEY)

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

print("üöÄ Starting STAGE 1: The Extractor...")

if not os.path.exists(ROOT_DIR):
    print(f"‚ùå Error: Root directory not found: {ROOT_DIR}")
else:
    project_folders = [f.path for f in os.scandir(ROOT_DIR) if f.is_dir()]

    session_input_tokens = 0
    session_output_tokens = 0

    for project_path in project_folders:
        project_folder_name = os.path.basename(project_path)

        # 1. EXTRACT PROJECT ID
        match = re.search(r'\d{2}-\d{2}-\d{3}', project_folder_name)
        if match:
            project_id = match.group(0)
        else:
            project_id = project_folder_name.replace(" ", "_")

        # 2. DEFINE FILES (STAGE 1 OUTPUT)
        crops_json_name = f"crops_project_{project_id}.json"
        crops_json_path = os.path.join(project_path, crops_json_name)
        crops_dir = os.path.join(project_path, "crops")

        if not os.path.exists(crops_dir):
            print(f"‚è© Skipping {project_folder_name}: No 'crops' folder.")
            continue

        print(f"\n--- Processing Project: {project_folder_name} ({project_id}) ---")

        # 3. LOAD OR CREATE PARTIAL FILE (Resume Capability)
        if os.path.exists(crops_json_path):
            with open(crops_json_path, 'r') as f:
                try:
                    current_data = json.load(f)
                except json.JSONDecodeError:
                    current_data = {"images": []}
        else:
            current_data = {"images": []}

        # 4. IDENTIFY REMAINING CROPS
        existing_ids = {img.get("image_id") for img in current_data.get("images", [])}
        all_crops = sorted(glob.glob(os.path.join(crops_dir, "*.png")))

        crops_to_process = []
        for crop_path in all_crops:
            fname = os.path.basename(crop_path)
            if fname not in existing_ids:
                crops_to_process.append(crop_path)

        if not crops_to_process:
            print("   ‚úÖ All crops already processed.")
            continue

        print(f"   üì∏ Found {len(crops_to_process)} new crops to process.")

        # 5. BATCH LOOP
        for i in range(0, len(crops_to_process), BATCH_SIZE):
            batch_files = crops_to_process[i : i + BATCH_SIZE]
            print(f"   ‚öôÔ∏è Processing Batch {i//BATCH_SIZE + 1} ({len(batch_files)} images)...")

            user_content = [
                {"type": "text", "text": "Analyze these images and output the 'images' JSON array."}
            ]

            for crop_path in batch_files:
                base64_img = encode_image(crop_path)
                fname = os.path.basename(crop_path)
                user_content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{base64_img}", "detail": "high"}
                })
                user_content.append({"type": "text", "text": f"Image ID: {fname}"})

            try:
                # API CALL
                response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": GOLDEN_PROMPT_PART_1},
                        {"role": "user", "content": user_content}
                    ],
                    max_tokens=16384,
                    temperature=0.2,
                    response_format={"type": "json_object"}
                )

                # SAVE INCREMENTALLY
                result_text = response.choices[0].message.content
                result_json = json.loads(result_text)
                new_images = result_json.get("images", [])

                current_data["images"].extend(new_images)

                with open(crops_json_path, "w") as f:
                    json.dump(current_data, f, indent=2)

                # TOKEN LOGGING
                usage = response.usage
                in_tokens = usage.prompt_tokens
                out_tokens = usage.completion_tokens
                session_input_tokens += in_tokens
                session_output_tokens += out_tokens

                print(f"      ‚úÖ Saved {len(new_images)} images.")
                print(f"      üí∞ Cost: Input {in_tokens} + Output {out_tokens} = {usage.total_tokens} Total")

            except Exception as e:
                print(f"      ‚ùå Batch failed: {e}")
                break

    print(f"\n‚ú® CELL 2.1 COMPLETE.")
    print(f"üìä SESSION TOTALS: Input: {session_input_tokens} | Output: {session_output_tokens}")

# @title CELL 2.2: THE ENGINEER (JSON -> Logic/Overview)
import json
import os
from openai import OpenAI
import re

# ================= CONFIGURATION =================
API_KEY = os.getenv("OPENAI_API_KEY", "YOUR_API_KEY_HERE")  # Set OPENAI_API_KEY environment variable or replace with your key
ROOT_DIR = "/content/drive/MyDrive/Projects Master JSON/dataset"

# =================================================
# üìã PASTE YOUR "GOLDEN PROMPT 3 - PART 2" BELOW
# =================================================
GOLDEN_PROMPT_PART_2 = r"""
You are a senior structural engineer and a logical reasoning model.
You are provided with a complete "images" array (generated in a previous step), which contains detailed text extraction and semantic data for every drawing crop in the project.

Your task is to analyze this existing data to build the project's connectivity, structural logic, and high-level summary.

**PROCESS YOU MUST FOLLOW:**

1. **First, Build Section 2 (Connectivity):**
   - Scan the "images" array to identify which crops belong to which **Page**.
   - Create the "Page Registry" to group them (e.g., grouping the Title Block crop with the Plan crop).
   - Establish the "Connectivity Matrix" by finding links between crops (e.g., match lines, detail callouts, or matching grid lines).

2. **Second, Build Section 3 (Synthesis):**
   - Using the connections you just found, trace the load paths from roof to foundation.
   - Describe the exterior and interior structural systems as continuous elements, not isolated crops.

3. **Third, Build Section 4 (Overview):**
   - Summarize the global project statistics (Design Criteria, Loads, Codes).
   - Generate the "Page Index Manifest" based on your findings in Section 2.

**OUTPUT:**
Output a SINGLE JSON object containing ONLY the following three keys:
{
  "drawing_connectivity_index": { ... },
  "structural_synthesis": { ... },
  "project_overview": { ... }
}


====================================================================
SECTION 2: CONNECTIONS SCHEMA
====================================================================

You are now performing the "Connectivity Analysis".
Input: The array of "images" generated in Section 1.
Task: Stitch these isolated crops into a cohesive document structure.
Output a SINGLE JSON object with the key "drawing_connectivity_index".

drawing_connectivity_index": {
  // 1Ô∏è‚É£PART 1: THE PAGE REGISTRY
  // INSTRUCTION: Group all crops by their 'page_index'.
  // LOGIC: Only ONE crop per page typically contains the Title Block info (Code/Description).
  //        You must find that one crop, extract the info, and apply it to the WHOLE page group.
  "page_registry": [
      {
          "page_index": "<integer, e.g. 4>",
          "inferred_page_code": "<The sheet code found on this page (e.g. 'A2.01'). If none found on any crop, use null>",
          "inferred_page_description": "<The title found on this page (e.g. 'BASEMENT PLAN'). If none found, use null>",
          "crops_on_this_page": [
              "<filename_of_crop_1>",
              "<filename_of_crop_2>",
              "<filename_of_crop_3>"
          ]
      }
      // ... repeat for every page index found ...
  ],

  // 2Ô∏è‚É£PART 2: THE CONNECTIVITY MATRIX
  // INSTRUCTION: Create a connection object for EVERY relationship found.
  // This list should be EXHAUSTIVE. Capture strong explicit links AND subtle inferred links.
  // PRIORITIES FOR LINKING:
  // 1. Same Page Context (Highest Confidence): Crops on Page 4 are inherently related.
  // 2. Explicit Callouts (High Confidence): Section 1 found "See Detail 3/A5". Find the crop that IS 3/A5.
  // 3. System Logic (Medium Confidence): A "Roof Plan" crop connects to a "Roof Truss" crop.
  "connectivity_graph": [
      {
        // 1. IDENTIFY THE LINK
        "source_image_id": "<filename_A>",
        "source_page_index": <integer>,

        "target_image_id": "<filename_B>",
        "target_page_index": <integer>,

        // 2. CLASSIFY THE LINK
        "connection_type": "<Select one: 'Same Sheet Context' / 'Detail Callout' / 'Section Cut' / 'Vertical Load Path' / 'Lateral System Match' / 'Spatial Adjacency'>",
        "hierarchical_relationship": "<Select one: 'Peer-to-Peer' (e.g. Plan to Plan) / 'Parent-to-Child' (e.g. Plan to Detail) / 'Sequential' (e.g. Page 1 to Page 2)>",

        // 3. VALIDATE THE LINK
        "common_spatial_identifiers": ["<e.g. Both contain Grid Line A>", "<e.g. Both contain Room 'Kitchen'>", "<None>"],
        "confidence_score": "<High (Explicit Callout or Same Page) / Medium (Strong Visual Match) / Low (Inferred Logic)>",

        // 4. EXPLAIN THE LINK
        "reasoning": "<Be specific. e.g. 'Image A contains bubble 3/S1. Image B is on Page S1 and is labeled Detail 3.' OR 'Image A contains a bubble 'See 5/S2'. Image B is on Page S2 and shows the visual characteristics of a footing detail.' OR 'Image A is the Beam Schedule for the Plan in Image B' OR 'Image A shows the grid line A-B which continues in Image B' OR 'Image A is the Title Block defining the context for Image B'>"
      }
      // ... Expecting a long list here ...
  ]
}

====================================================================
SECTION 3: STRUCTURAL SYNTHESIS SCHEMA
====================================================================

Input: The 'images' array (Section 1) and 'drawing_connectivity_index' (Section 2).
Task: Synthesize the comprehensive structural logic of the entire building.
Output a SINGLE JSON object with the key "structural_synthesis".

"structural_synthesis": {

    // 1Ô∏è‚É£ EXTERIOR PERIMETER SYSTEM (The Shell)
    // INSTRUCTION: Walk the building perimeter. Summarize the dominant structural systems
    // for each elevation. If geometry is complex (steps/jogs), summarize the "System" rather than listing every corner.
    "exterior_perimeter_walkthrough": {
        "plan_north_perimeter": {
            "description": "<e.g. Continuous 10-inch ICF foundation wall stepping down at East corner.>",
            "identified_assembly_tags": ["<e.g. EW1>", "<e.g. ICF1>"],
            "foundation_condition": "<e.g. Strip Footing / Stepped Footing / Slab Edge>"
        },
        "plan_east_perimeter": {
            "description": "<e.g. Wood framed walkout wall with large window openings.>",
            "identified_assembly_tags": ["<e.g. EW2>"],
            "foundation_condition": "<e.g. Frost Wall>"
        },
        "plan_south_perimeter": {
            "description": "<...>",
            "identified_assembly_tags": [],
            "foundation_condition": "<...>"
        },
        "plan_west_perimeter": {
            "description": "<...>",
            "identified_assembly_tags": [],
            "foundation_condition": "<...>"
        }
    },

    // 2Ô∏è‚É£ INTERIOR STRUCTURAL ZONES (The Core)
    // INSTRUCTION: Break the interior into logical structural zones (e.g. Main Span, Garage, Addition).
    "interior_structural_zones": [
        {
            "zone_name": "<e.g. MAIN FLOOR - OPEN LIVING AREA>",
            "horizontal_framing": "<e.g. TJI Joists spanning North-South @ 16\" o.c.>",
            "vertical_supports": "<e.g. Central flush steel beam (B1) supported by 3-ply wood posts.>",
            "span_notes": "<e.g. Joists appear to span approx 14ft from exterior wall to central beam.>"
        },
        {
            "zone_name": "<e.g. GARAGE / UNHEATED>",
            "horizontal_framing": "<e.g. Pre-engineered roof trusses @ 24\" o.c.>",
            "vertical_supports": "<e.g. Bearing on exterior 2x6 walls. No interior posts.>",
            "span_notes": "<e.g. Clear span approx 22ft.>"
        }
    ],

    // 3Ô∏è‚É£ GLOBAL LOAD TRANSFER (The Logic)
    // INSTRUCTION: Trace the primary loads from sky to ground.
    "load_transfer_mechanisms": {
        "gravity_load_trace": [
            "<Step 1: Roof Loads (Trusses) -> Bearing Walls>",
            "<Step 2: Main Floor Loads (Joists) -> Central Beam & Ext Walls>",
            "<Step 3: Beam Loads -> Steel Columns -> Pad Footings>",
            "<Step 4: Wall Loads -> Strip Footings -> Undisturbed Soil>"
        ],
        "lateral_load_strategy": "<e.g. Rigid Diaphragms (Floors) transfer wind to Exterior Shear Walls (ICF/Wood).>",

        // CRITICAL: Identify where load paths get complicated
        "major_point_loads_or_transfers": [
            "<e.g. Girder Truss GT1 point load transfers to Post P1.>",
            "<e.g. Steel Beam B1 pocketed into Foundation Wall at Grid A.>"
        ]
    }
}


====================================================================
SECTION 4: PROJECT OVERVIEW SCHEMA
====================================================================

Input: All data from Sections 1, 2, and 3.
Task: Generate the "Header" metadata that summarizes the entire project.
Output a SINGLE JSON object with the key "project_overview".


"project_overview": {

    // 1Ô∏è‚É£ IDENTITY & DOCUMENT CONTROL
    // The "Title Block" data for the whole project.
    "project_identity": {
        "project_name": "<from title block>",
        "project_address": "<full address if available>",
        "client_name": "<from title block>",
        "project_number": "<e.g. 25-01-039>",
        "design_firm": "<Architect/Engineer Company Name>",
        "document_status": "<e.g. ISSUED FOR PERMIT / CONSTRUCTION / PRELIMINARY>",
        "project_units": "<e.g. Imperial (ft/in) / Metric (mm) / Mixed>",
        "latest_revision_date": "<Most recent date found across all pages>"
    },

    // 2Ô∏è‚É£ PAGE INDEX MANIFEST (The Table of Contents)
    // INSTRUCTION: Look at your Section 2 'page_registry'. List every unique page found.
    "page_index_manifest": [
        {
            "page_code": "<e.g. A2.01 (matches 'inferred_page_code' from Sec 2)>",
            "page_description": "<e.g. BASEMENT PLAN (matches 'inferred_page_description' from Sec 2)>",
            "page_index_reference": <integer>
        }
        // ... list all pages ...
    ],

    // 3Ô∏è‚É£ REGULATORY & DESIGN CRITERIA (The Inputs)
    "design_criteria": {
        "applicable_codes": ["<e.g. OBC 2012>", "<e.g. NBCC 2020>"],
        "design_loads": {
            "snow_load": "<e.g. Ss=1.2 kPa, Sr=0.4 kPa>",
            "wind_load": "<e.g. q1/50 = 0.45 kPa>",
            "floor_live_load": "<e.g. 40 psf (Residential), 100 psf (Assembly)>",
            "soil_bearing_capacity": "<e.g. 75 kPa / 1500 psf>"
        },
        "seismic_data": {
            "site_class": "<e.g. Class C / D>",
            "importance_category": "<e.g. Normal / High>"
        }
    },

    // 4Ô∏è‚É£ GLOBAL GEOMETRY & STATS (The Shape)
    "building_statistics": {
        "building_type_classification": "<e.g. Single Family Residential / Agricultural Barn / Industrial Warehouse>",
        "number_of_stories": "<e.g. 1 Storey + Basement>",
        "approximate_footprint": "<e.g. 50' x 40'>",
        "roof_geometry": "<e.g. Gable Roof, 12/12 Pitch, Complex Hip>"
    },

    // 5Ô∏è‚É£ STRUCTURAL SYSTEMS SUMMARY (The Skeleton)
    // Summarize the data synthesized in Section 3.
    "structural_systems": {
        "foundation_system": {
            "type": "<e.g. Strip Footings & ICF Walls>",
            "key_notes": "<e.g. Stepped footings required at walkout>"
        },
        "floor_system": {
            "framing_type": "<e.g. TJI Joists / 2x10 Dimensional Lumber / Slab-on-Grade>",
            "sheathing": "<e.g. 3/4\" T&G Plywood glued & screwed>"
        },
        "wall_system": {
            "exterior": "<e.g. 2x6 Wood Stud @ 16\" o.c.>",
            "interior_bearing": "<e.g. 2x4 Wood Stud>",
            "special_walls": "<e.g. Tall walls at Great Room (Steel columns embedded)>"
        },
        "roof_system": {
            "primary_framing": "<e.g. Pre-engineered Wood Trusses>",
            "special_framing": "<e.g. Steel Ridge Beam W8x15 for Vaulted Ceiling>",
            "overhangs": "<e.g. 24\" Typical Overhang>"
        },
        "lateral_system": {
            "primary_resistance": "<e.g. Wood Shear Walls / Moment Frames / ICF Core>",
            "diaphragm_type": "<e.g. Plywood Roof & Floor Diaphragms>"
        }
    },

    // 6Ô∏è‚É£ MATERIAL PALETTE (Global Specs)
    "material_specifications_summary": {
        "concrete_strength": "<e.g. 20 MPa (Footings), 32 MPa (Walls)>",
        "lumber_grade": "<e.g. SPF No. 1/2>",
        "steel_grade": "<e.g. ASTM A992 (Beams), A500 Gr. C (HSS)>",
        "engineered_wood": "<e.g. LVL 2.0E, PSL, TJI>"
    },

    // 7Ô∏è‚É£ HIGH-LEVEL SUMMARY
    "summary_for_embedding": "<Comprehensive 3-5 sentence paragraph describing the project scope, structural logic, and key engineering challenges.>"
}

"""
# =================================================

client = OpenAI(api_key=API_KEY)

print("üöÄ Starting STAGE 2: The Engineer...")

if not os.path.exists(ROOT_DIR):
    print(f"‚ùå Error: Root directory not found: {ROOT_DIR}")
else:
    project_folders = [f.path for f in os.scandir(ROOT_DIR) if f.is_dir()]

    session_input_tokens = 0
    session_output_tokens = 0

    for project_path in project_folders:
        project_folder_name = os.path.basename(project_path)

        # 1. IDENTIFY FILES
        match = re.search(r'\d{2}-\d{2}-\d{3}', project_folder_name)
        project_id = match.group(0) if match else project_folder_name.replace(" ", "_")

        # INPUT FILE (From Stage 1)
        crops_json_name = f"crops_project_{project_id}.json"
        crops_json_path = os.path.join(project_path, crops_json_name)

        # OUTPUT FILE (Stage 2 Final)
        master_json_name = f"master_project_{project_id}.json"
        master_json_path = os.path.join(project_path, master_json_name)

        # 2. VALIDATION
        if not os.path.exists(crops_json_path):
            print(f"‚è© Skipping {project_folder_name}: No '{crops_json_name}' found (Run Cell 2.1 first).")
            continue

        if os.path.exists(master_json_path):
            print(f"‚è© Skipping {project_folder_name}: '{master_json_name}' already exists.")
            continue

        print(f"\n--- Synthesizing Project: {project_folder_name} ---")

        # 3. LOAD DATA (TEXT ONLY)
        with open(crops_json_path, 'r') as f:
            stage1_data = json.load(f)

        images_array = stage1_data.get("images", [])
        if not images_array:
            print("   ‚ö†Ô∏è No images found in Stage 1 file. Skipping.")
            continue

        # 4. PREPARE PAYLOAD
        # We dump the entire Stage 1 JSON array into a string.
        json_text_context = json.dumps(images_array, indent=2)

        user_content = f"Here is the 'images' array extracted from the visual analysis:\n\n{json_text_context}"

        # 5. API CALL
        try:
            print(f"   üß† Reasoning on {len(images_array)} data points...")

            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": GOLDEN_PROMPT_PART_2},
                    {"role": "user", "content": user_content}
                ],
                max_tokens=16384, # Large buffer for the huge "Master" output
                temperature=0.2,
                response_format={"type": "json_object"}
            )

            # 6. SAVE MASTER FILE
            result_text = response.choices[0].message.content
            result_json = json.loads(result_text)

            # Save strictly the 3 keys requested
            with open(master_json_path, "w") as f:
                json.dump(result_json, f, indent=2)

            # TOKEN LOGGING
            usage = response.usage
            in_tokens = usage.prompt_tokens
            out_tokens = usage.completion_tokens
            session_input_tokens += in_tokens
            session_output_tokens += out_tokens

            print(f"   ‚úÖ Master File Created: {master_json_name}")
            print(f"      üí∞ Cost: Input {in_tokens} + Output {out_tokens} = {usage.total_tokens} Total")

        except Exception as e:
            print(f"   ‚ùå Synthesis Failed: {e}")

    print(f"\n‚ú® CELL 2.2 COMPLETE.")
    print(f"üìä SESSION TOTALS: Input: {session_input_tokens} | Output: {session_output_tokens}")

# @title CELL 3: MULTI-PROJECT JSON MERGER
import json
import csv

output_csv_path = os.path.join(ROOT_DIR, "training.csv")
all_rows = []

print("üöÄ Starting Merger...")
project_folders = [f.path for f in os.scandir(ROOT_DIR) if f.is_dir()]

for project_path in project_folders:
    project_name = os.path.basename(project_path)
    json_path = os.path.join(project_path, f"{project_name}_analysis.json")

    if not os.path.exists(json_path):
        continue

    print(f"   Processing JSON for: {project_name}")

    try:
        with open(json_path, 'r') as f:
            data_obj = json.load(f)

        # Parse Golden Prompt 2 Structure
        # It expects "images" array and "image_relationship_graph" in "project_overview" or root

        images_data = data_obj.get("images", [])

        # Try to find relationship graph (could be in root or project_overview)
        rels_graph = data_obj.get("image_relationship_graph", [])
        if not rels_graph:
            rels_graph = data_obj.get("project_overview", {}).get("image_relationship_graph", [])

        # 1. PROCESS IMAGES (Captions)
        for img in images_data:
            # Get Filename (Clean up if LLM added text)
            raw_id = img.get("image_id", "")
            # If the LLM just said "crop_0.png", we need to know which project it belongs to?
            # Actually, the file stays in the project folder. We just need the filename.
            filename = raw_id.replace("Image ID:", "").strip()

            # Construct Training Path: "Project Name/crops/filename"
            # This is CRITICAL for Florence to find the file later
            relative_path = f"{project_name}/crops/{filename}"

            # Build Caption
            desc = img.get("description", "")
            # Extract components for richer caption
            struct_elems = img.get("detected_components", {}).get("structural_elements", [])
            elems_str = ", ".join([x.get("type", "elem") for x in struct_elems])

            full_caption = f"{desc} Contains: {elems_str}."

            all_rows.append([relative_path, "caption", full_caption])

            # Local Relationships (from spatial_context)
            local_rels = img.get("spatial_context", {}).get("relationship_to_other_images", [])
            for rel in local_rels:
                if rel.get("reference_found", "no").lower() == "yes":
                    target_id = rel.get("related_image_id", "")
                    target_path = f"{project_name}/crops/{target_id}"
                    all_rows.append([relative_path, "relation", target_path])

        # 2. PROCESS GLOBAL GRAPH (Relationships)
        for link in rels_graph:
            src_id = link.get("from_image_id", "") or link.get("from_image", "")
            dst_id = link.get("to_image_id", "") or link.get("to_image", "")

            if src_id and dst_id:
                src_path = f"{project_name}/crops/{src_id}"
                dst_path = f"{project_name}/crops/{dst_id}"
                all_rows.append([src_path, "relation", dst_path])

    except Exception as e:
        print(f"   ‚ùå Error reading JSON for {project_name}: {e}")

# Save CSV
with open(output_csv_path, 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["image_path", "task_type", "text_output"]) # Header
    writer.writerows(all_rows)

print(f"\n‚ú® PHASE 3 COMPLETE: Generated {len(all_rows)} training rows.")
print(f"Saved to: {output_csv_path}")

# @title CELL 4: CSV TO JSONL
import pandas as pd
import json

csv_path = os.path.join(ROOT_DIR, "training.csv")
output_jsonl = os.path.join(ROOT_DIR, "annotations.jsonl")

if not os.path.exists(csv_path):
    print("‚ùå CSV not found. Run Cell 3 first.")
else:
    df = pd.read_csv(csv_path)

    with open(output_jsonl, 'w') as f:
        for index, row in df.iterrows():
            path = row['image_path'] # This is "ProjectName/crops/filename.png"
            task = str(row['task_type']).lower()
            text = str(row['text_output'])

            prefix = "<MORE_DETAILED_CAPTION>"
            if "relation" in task:
                prefix = "<RELATED_TO>"

            entry = {
                "image": path,
                "prefix": prefix,
                "suffix": text
            }
            f.write(json.dumps(entry) + '\n')

    print(f"\n‚ú® PHASE 4 COMPLETE: {output_jsonl} created.")
    print("üöÄ YOU ARE READY TO TRAIN.")